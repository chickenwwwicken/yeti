There are a lot of existing algorithms; some are fast and some are slow. Some use lots of memory. It can be hard to decide which algo is the best to solve a particular problem.

"Big O" analysis is one way to compare the practicality of algorithms by classifying their time complexity. 

Big  O is a characterization of algos according to their worst-case growth rates. 

We write Big-O notation like this:
```
0(formula)
```
Where `formula` describes hwo an algo's runtime or space requirements grow as the input size grows. 
- `O(1)` - constant
- `O(n)` - linear
- `O(n^2)` - squared
- `O(2^n)` - exponential
- `O(n!)` - factorial
The following chart shows the growth rate of several different Big O categories. The size of the input is shown on the `x axis` and how long the algorithm will take to complete is shown on the `y axis`. 
![[Pasted image 20250416113616.jpg]]

As the size of inputs grows, the algos become slower to complete (take longer to run). The rate at which they become slower is defined by their Big O category. 

For example, `O(n)` algos slow down more slowly than `0(n^2)` algos

### The Worst Big-O category?
The algorithms that slow down the fastest in our chart are the factorial and exponential algorithms, or `0(n!),` and `0(2^n)`.

---
[[3.0_big-o]]
[[3.2_o(n)]]